{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h2>Project 3: Na&iuml;ve Bayes</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<blockquote>\n",
    "    <center>\n",
    "    <img src=\"nb.png\" width=\"200px\" />\n",
    "    </center>\n",
    "      <p><cite><center>\"All models are wrong, but some are useful.\"<br>\n",
    "       -- George E.P. Box\n",
    "      </center></cite></p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3>Introduction</h3>\n",
    "<!--AÃ°albrandr-->\n",
    "\n",
    "<p> You recently decided that you want to take the machine learning course at Cornell University. You're super excited to learn ML but are very confused during lectures. You realize that it's because your German professor just throws in German words while explaining concepts and you have no idea what these mean! You could just tell him you don't understand German but you're a proud engineer! There's got to be a better way. Something that doesn't involve communicating because, you know, that's not our forte (just kidding). So, you decide to create a system that detects every time a word is German and translates it for you in the subtitles. In this project, you will just implement the first part of this system using Na&iuml;ve Bayes to predict if a word is German or English. </p>\n",
    "<p>\n",
    "<strong>P3 Deadlines:</strong> \n",
    "The deadline for this project is on <strong>October 3 (11:59 pm EST)</strong>. The late deadline is on <strong>October 5</strong>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> English and German words </h3>\n",
    "\n",
    "<p> Take a look at the files <code>english_train.txt</code> and <code>german_train.txt</code>. For example with the unix command <pre>cat german_train.txt</pre> \n",
    "<pre>\n",
    "...\n",
    "bibliothek\n",
    "aufzuhalten\n",
    "maegde\n",
    "rupfen\n",
    "leer\n",
    "merkte\n",
    "sucht\n",
    "launenhaften\n",
    "graeten\n",
    "</pre>\n",
    "\n",
    "The problem with the current file is that the words are in plain text, which makes it hard for a machine learning algorithm to learn anything useful from them. We therefore need to transform them into some vector format, where each word becomes a vector that represents a point in some high dimensional input space. </p>\n",
    "\n",
    "<p>That is exactly what the following Python function <code>language2features</code> does: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "import numpy as np\n",
    "import sys\n",
    "from cvxpy import *\n",
    "from matplotlib import pyplot as plt\n",
    "#</GRADED>\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def feature_extraction_letters(word, B):\n",
    "    v = np.zeros(B)\n",
    "    for letter in word:\n",
    "        v[ord(letter) - 97] += 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def language2features(filename, B=26, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "    X : n feature vectors of dimension B, (nxB)\n",
    "    \"\"\"\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            words = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        words = filename.split('\\n')\n",
    "    n = len(words)\n",
    "    X = np.zeros((n, B))\n",
    "    for i in range(n):\n",
    "        X[i,:] = feature_extraction_letters(words[i].lower(), B)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>It reads every word in the given file and converts it into a 26-dimensional feature vector by mapping each letter to a feature. The generated vector is a histogram containing the number of occurrences of each letter in the word.</p> \n",
    "\n",
    "<p>We have provided you with a python function <code>genFeatures</code>, which calls this function, transforms the words into features and loads them into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def genFeatures(dimension, language2features, file_german, file_english):\n",
    "    \"\"\"\n",
    "    function [x,y]=genFeatures\n",
    "    \n",
    "    This function calls \"language2features.py\" to convert \n",
    "    words into feature vectors and load training data. \n",
    "    \n",
    "    language2features: function that extracts features from language word\n",
    "    dimension: dimensionality of the features\n",
    "    \n",
    "    Output: \n",
    "    x: n feature vectors of dimensionality d [n,d]\n",
    "    y: n labels (-1 = German, +1 = English)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load in the data\n",
    "    Xgerman = language2features(file_german, B=dimension)\n",
    "    Xenglish = language2features(file_english, B=dimension)\n",
    "    X = np.concatenate([Xgerman, Xenglish])\n",
    "    \n",
    "    # Generate Labels\n",
    "    Y = np.concatenate([-np.ones(len(Xgerman)), np.ones(len(Xenglish))])\n",
    "    \n",
    "    # shuffle data into random order\n",
    "    ii = np.random.permutation([i for i in range(len(Y))])\n",
    "    \n",
    "    return X[ii, :], Y[ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You can call the following command to load features and labels of all German and English words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X,Y = genFeatures(26, language2features, \"german_train.txt\", \"english_train.txt\")\n",
    "xTe, yTe = genFeatures(26, language2features, \"german_test.txt\", \"english_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> Multinomial Na&iuml;ve Bayes Classifier </h3>\n",
    "\n",
    "<p> The Na&iuml;ve Bayes classifier is a linear classifier based on Bayes Rule. The following questions will ask you to finish these functions in a pre-defined order. <br></p>\n",
    "<p>(a) Estimate the class probability P(Y) in \n",
    "<b><code>naivebayesPY</code></b>\n",
    ". This should return the probability that a sample in the training set is positive or negative, independent of its features.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#<GRADED>\n",
    "def naivebayesPY(x,y):\n",
    "    \"\"\"\n",
    "    function [pos,neg] = naivebayesPY(x,y);\n",
    "\n",
    "    Computation of P(Y)\n",
    "    Input:\n",
    "        x : n input vectors of d dimensions (n,d)\n",
    "        y : n labels (-1 or +1) (n,)\n",
    "\n",
    "    Output:\n",
    "    pos: probability p(y=1)\n",
    "    neg: probability p(y=-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    pos_list = [1 if ys == 1 else 0 for ys in y]\n",
    "    pos = np.sum(pos_list)/len(y)\n",
    "    neg_list = [1 if ys == -1 else 0 for ys in y]\n",
    "    neg = np.sum(neg_list)/len(y)\n",
    "    \n",
    "    return pos, neg\n",
    "\n",
    "#</GRADED>\n",
    "\n",
    "pos,neg = naivebayesPY(X,Y)\n",
    "neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(b) Estimate the conditional probabilities P(X|Y) <b>(Maximum Likelihood Estimate)</b> without smoothing in \n",
    "<b><code>naivebayesPXY_mle</code></b>\n",
    ".  Use a <b>multinomial</b> distribution as model. This will return the probability vectors  for all features given a class label.\n",
    "Stated differently, the output vector should have dimensions equal to the number of letters of the alphabet. For each index i, we ask you to find the probability that any letter of any word contains this ith letter of the alphabet.  \n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08078045, 0.01732312, 0.04868709, 0.03501094, 0.11579139,\n",
       "       0.01495259, 0.01914661, 0.02297593, 0.08698031, 0.00182349,\n",
       "       0.00948213, 0.05634573, 0.02881109, 0.07239241, 0.06363968,\n",
       "       0.03081692, 0.00164114, 0.07640408, 0.0607221 , 0.07749818,\n",
       "       0.03136397, 0.01331145, 0.01002918, 0.00273523, 0.02005835,\n",
       "       0.00127644])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#<GRADED>\n",
    "def naivebayesPXY_mle(x,y):\n",
    "    \"\"\"\n",
    "    function [posprob,negprob] = naivebayesPXY(x,y);\n",
    "    \n",
    "    Computation of P(X|Y) -- Maximum Likelihood Estimate\n",
    "    Input:\n",
    "        x : n input vectors of d dimensions (n,d)\n",
    "        y : n labels (-1 or +1) (n,)\n",
    "    \n",
    "     Output:\n",
    "    posprob: probability vector of p(x=ith letter of alphabet|y=1) (d,)\n",
    "    negprob: probability vector of p(x=ith letter of alphabet|y=-1) (d,)\n",
    "    \"\"\"\n",
    "    \n",
    "    pos_indices = np.where(y == 1)[0]\n",
    "    neg_indices = np.where(y == -1)[0]\n",
    "    \n",
    "    n, d = x.shape\n",
    "    posprob = np.zeros(d)\n",
    "    negprob = np.zeros(d)\n",
    "    \n",
    "    for i in range(d):\n",
    "        pos_count = np.sum(x[pos_indices, i])\n",
    "        neg_count = np.sum(x[neg_indices, i])\n",
    "        \n",
    "        total_pos_count = np.sum(x[pos_indices])\n",
    "        total_neg_count = np.sum(x[neg_indices])\n",
    "        \n",
    "        posprob[i] = (pos_count) / (total_pos_count)  \n",
    "        negprob[i] = (neg_count) / (total_neg_count)  \n",
    "    \n",
    "    return posprob.flatten(), negprob.flatten()\n",
    "    \n",
    "    \n",
    "#</GRADED>\n",
    "\n",
    "posprob_mle,negprob_mle = naivebayesPXY_mle(X,Y)\n",
    "posprob_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(c) Estimate the conditional probabilities P(X|Y) <b>(Smoothing with Laplace estimate)</b> in \n",
    "<b><code>naivebayesPXY_smoothing</code></b>\n",
    ".  Use a <b>multinomial</b> distribution as model. This will return the probability vectors  for all features given a class label.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08058076, 0.01742287, 0.04863884, 0.03502722, 0.1154265 ,\n",
       "       0.01506352, 0.01923775, 0.023049  , 0.08675136, 0.00199637,\n",
       "       0.00961887, 0.05626134, 0.02885662, 0.0722323 , 0.06352087,\n",
       "       0.03085299, 0.00181488, 0.07622505, 0.06061706, 0.07731397,\n",
       "       0.03139746, 0.01343013, 0.01016334, 0.00290381, 0.02014519,\n",
       "       0.00145191])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#<GRADED>\n",
    "def naivebayesPXY_smoothing(x,y):\n",
    "    \"\"\"\n",
    "    function [posprob,negprob] = naivebayesPXY(x,y);\n",
    "    \n",
    "    Computation of P(X|Y) -- Smoothing with Laplace estimate\n",
    "    Input:\n",
    "        x : n input vectors of d dimensions (n,d)\n",
    "        y : n labels (-1 or +1) (n,)\n",
    "    \n",
    "    Output:\n",
    "    posprob: probability vector of p(x|y=1) (d,)\n",
    "    negprob: probability vector of p(x|y=-1) (d,)\n",
    "    \"\"\"\n",
    "  \n",
    "    n, d = x.shape\n",
    "    \n",
    "    pos_indices = np.where(y == 1)[0]\n",
    "    neg_indices = np.where(y == -1)[0]\n",
    "    \n",
    "    posprob = np.ones(d)  \n",
    "    negprob = np.ones(d)  \n",
    "    \n",
    "    for i in range(d):\n",
    "        pos_count = np.sum(x[pos_indices, i])\n",
    "        neg_count = np.sum(x[neg_indices, i])\n",
    "        \n",
    "        total_pos_count = np.sum(x[pos_indices])\n",
    "        total_neg_count = np.sum(x[neg_indices])\n",
    "        \n",
    "        posprob[i] = (pos_count + 1) / (total_pos_count + d)  \n",
    "        negprob[i] = (neg_count + 1) / (total_neg_count + d)  \n",
    "    \n",
    "    return posprob, negprob\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#</GRADED>\n",
    "\n",
    "posprob_smoothing,negprob_smoothing = naivebayesPXY_smoothing(X,Y)\n",
    "posprob_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## <p>(d) Solve for the log ratio, $\\log\\left(\\frac{P(Y=1 | X = xtest)}{P(Y=-1|X= xtest)}\\right)$, using Bayes Rule. Doing this in log space is important to avoid underflow. Make sure to first log the probabilities and then sum them.\n",
    " Implement this in \n",
    "<b><code>naivebayes</code></b>.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "#<GRADED>\n",
    "def naivebayes(x,y,xtest,naivebayesPXY):\n",
    "    \"\"\"\n",
    "    function logratio = naivebayes(x,y);\n",
    "    \n",
    "    Computation of log P(Y|X=x1) using Bayes Rule\n",
    "    Input:\n",
    "    x : n input vectors of d dimensions (n,d)\n",
    "    y : n labels (-1 or +1)(n,)\n",
    "    xtest: input vector of d dimensions (d,)\n",
    "    naivebayesPXY: input function for getting conditional probabilities (naivebayesPXY_smoothing OR naivebayesPXY_mle)\n",
    "    \n",
    "    Output:\n",
    "    logratio: log (P(Y = 1|X=xtest)/P(Y=-1|X=xtest))\n",
    "    \"\"\"\n",
    "    \n",
    "    posprob, negprob = naivebayesPXY(x, y)\n",
    "    \n",
    "    pos_list = [1 if ys == 1 else 0 for ys in y]\n",
    "    prior_pos = np.sum(pos_list)/len(y)\n",
    "    neg_list = [1 if ys == -1 else 0 for ys in y]\n",
    "    prior_neg = np.sum(neg_list)/len(y)\n",
    "    \n",
    "\n",
    "    pos_log = 1.0\n",
    "    neg_log = 1.0\n",
    "    for i in range(len(xtest)):\n",
    "        pos_log *= (posprob[i]**xtest[i])\n",
    "        neg_log *= (negprob[i]**xtest[i])\n",
    "    \n",
    "    CP_pos = np.log(pos_log) + np.log(prior_pos)\n",
    "    CP_neg = np.log(neg_log) + np.log(prior_neg)\n",
    "    logratio = (CP_pos - CP_neg)\n",
    "    \n",
    "    \n",
    "    return logratio\n",
    "    \n",
    "#</GRADED>\n",
    "p_smoothing = naivebayes(X,Y,X[0,:], naivebayesPXY_smoothing)\n",
    "p_mle = naivebayes(X,Y,X[0,:], naivebayesPXY_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(e) NaÃ¯ve Bayes can also be written as a linear classifier. Implement this in \n",
    "<b><code>naivebayesCL</code></b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def naivebayesCL(x,y,naivebayesPXY):\n",
    "    \"\"\"\n",
    "    function [w,b]=naivebayesCL(x,y);\n",
    "    Implementation of a Naive Bayes classifier\n",
    "    Input:\n",
    "    x : n input vectors of d dimensions (n,d)\n",
    "    y : n labels (-1 or +1)(n,)\n",
    "    naivebayesPXY: input function for getting conditional probabilities (naivebayesPXY_smoothing OR naivebayesPXY_mle)\n",
    "\n",
    "    Output:\n",
    "    w : weight vector of d dimensions (d,)\n",
    "    b : bias (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    n, d = x.shape\n",
    "    posprob, negprob = naivebayesPXY(x, y)\n",
    "    \n",
    "    pos_list = [1 if ys == 1 else 0 for ys in y]\n",
    "    prior_pos = np.sum(pos_list)/len(y)\n",
    "    neg_list = [1 if ys == -1 else 0 for ys in y]\n",
    "    prior_neg = np.sum(neg_list)/len(y)\n",
    "    \n",
    "    w = np.log(posprob / negprob)\n",
    "    \n",
    "    \n",
    "    b = np.log(prior_pos/prior_neg)\n",
    "    return w, b\n",
    "\n",
    "\n",
    "#</GRADED>\n",
    "\n",
    "w_smoothing,b_smoothing = naivebayesCL(X,Y, naivebayesPXY_smoothing)\n",
    "w_mle,b_mle = naivebayesCL(X,Y, naivebayesPXY_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(f) Implement \n",
    "<b><code>classifyLinear</code></b>\n",
    " that applies a linear weight vector and bias to a set of input vectors and outputs their predictions.  (You can use your answer from the previous project.)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error (Smoothing with Laplace estimate): 25.56%\n",
      "Training error (Maximum Likelihood Estimate): 25.44%\n",
      "Test error (Smoothing with Laplace estimate): 25.67%\n",
      "Test error (Maximum Likelihood Estimate): 25.50%\n"
     ]
    }
   ],
   "source": [
    "#<GRADED>\n",
    "def classifyLinear(x,w,b=0):\n",
    "    \"\"\"\n",
    "    function preds=classifyLinear(x,w,b);\n",
    "    \n",
    "    Make predictions with a linear classifier\n",
    "    Input:\n",
    "    x : n input vectors of d dimensions (n,d)\n",
    "    w : weight vector of d dimensions (d,)\n",
    "    b : bias (optional)\n",
    "    \n",
    "    Output:\n",
    "    preds: predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    w = w.flatten()    \n",
    "    n,_ = x.shape\n",
    "    preds = np.zeros(n)\n",
    "    preds += np.dot(x,w) + b\n",
    "    preds = np.sign(preds)  \n",
    "    return preds\n",
    "\n",
    "#</GRADED>\n",
    "\n",
    "print('Training error (Smoothing with Laplace estimate): %.2f%%' % (100 *(classifyLinear(X, w_smoothing, b_smoothing) != Y).mean()))\n",
    "print('Training error (Maximum Likelihood Estimate): %.2f%%' % (100 *(classifyLinear(X, w_mle, b_mle) != Y).mean()))\n",
    "print('Test error (Smoothing with Laplace estimate): %.2f%%' % (100 *(classifyLinear(xTe, w_smoothing, b_smoothing) != yTe).mean()))\n",
    "print('Test error (Maximum Likelihood Estimate): %.2f%%' % (100 *(classifyLinear(xTe, w_mle, b_mle) != yTe).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You can now test your code with the following interactive word classification script. Use the word 'exit' to stop the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Training classifier (Smoothing with Laplace estimate) ...\n",
      "Training error (Smoothing with Laplace estimate): 25.56%\n",
      "Test error (Smoothing with Laplace estimate): 25.67%\n",
      "exit\n",
      "exit, is an english word.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DIMS = 26\n",
    "print('Loading data ...')\n",
    "X,Y = genFeatures(DIMS, language2features, \"german_train.txt\", \"english_train.txt\")\n",
    "xTe, yTe = genFeatures(26, language2features, \"german_test.txt\", \"english_test.txt\")\n",
    "print('Training classifier (Smoothing with Laplace estimate) ...')\n",
    "w,b=naivebayesCL(X,Y,naivebayesPXY_smoothing)\n",
    "train_error = np.mean(classifyLinear(X,w,b) != Y)\n",
    "test_error = np.mean(classifyLinear(xTe,w,b) != yTe)\n",
    "print('Training error (Smoothing with Laplace estimate): %.2f%%' % (100 * train_error))\n",
    "print('Test error (Smoothing with Laplace estimate): %.2f%%' % (100 * test_error))\n",
    "\n",
    "yourword = \"\"\n",
    "while yourword!=\"exit\":\n",
    "    yourword = input()\n",
    "    if len(yourword) < 1:\n",
    "        break\n",
    "    xtest = language2features(yourword,B=DIMS,LoadFile=False)\n",
    "    pred = classifyLinear(xtest,w,b)[0]\n",
    "    if pred > 0:\n",
    "        print(\"%s, is an english word.\\n\" % yourword)\n",
    "    else:\n",
    "        print(\"%s, is a german word.\\n\" % yourword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> Feature Extraction</h3>\n",
    "\n",
    "<p> Similar to how we extracted features from words in <code>feature_extraction_letters</code>, we are going to try another way of doing so. This time, instead of mapping a letter to a feature, we will map a pair of letters to a feature. </p>\n",
    "    \n",
    "<p>\n",
    "Every element in the feature vector will represent a pair of letters (e.g. 'aa', 'ab', 'ac'...) and the element representing the pair of letters that occur in the word will be the number of occurence. Make sure your feature vector <b> ordering is alphabetical </b> i.e. ['aa', 'ab', 'ac'.....'ba', 'bb'......'ca','cb'......]. The length of the feature vector will be $26^2 = 676$ to represent all possible pairs of 26 letters. Assume everything is in lower case.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Here's an example, for the word 'mama', elements in the feature vector representing 'ma' will be 2, 'am' will be 1. All the other 674 features will be 0.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Please modify <code><b>feature_extraction_letters_pairs</b></code> below to implement this feature extraction.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def feature_extraction_letters_pairs(word, B=676):\n",
    "    \"\"\"\n",
    "    Feature extration from word for pairs\n",
    "    word: word of the language as a string\n",
    "    \n",
    "    Output:\n",
    "    v : a feature vectors of dimension B=676, (B,)\n",
    "    \"\"\"\n",
    "    v = np.zeros(B)\n",
    "    \n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    alphabet_pairs = []\n",
    "    for i in range(len(alphabet)):\n",
    "        for j in range(len(alphabet)):\n",
    "            pair = alphabet[i] + alphabet[j]\n",
    "            alphabet_pairs.append(pair)\n",
    "    \n",
    "    for j in range(len(word)-1):\n",
    "        idx = alphabet_pairs.index(word[j]+word[j+1])\n",
    "        v[idx] += 1\n",
    "\n",
    "\n",
    "    return v\n",
    "    \n",
    "def language2features_pairs(filename, B=676, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "    X : n feature vectors of dimension B, (n,B)\n",
    "    \"\"\"\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            words = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        words = filename.split('\\n')\n",
    "    n = len(words)\n",
    "    X = np.zeros((n, B))\n",
    "    for i in range(n):\n",
    "        X[i,:] = feature_extraction_letters_pairs(words[i].lower(), B)\n",
    "    return X\n",
    "\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extraction_letters_pairs('mama', B=676)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "alphabet_pairs = []\n",
    "for i in range(len(alphabet)):\n",
    "    for j in range(len(alphabet)):\n",
    "        pair = alphabet[i] + alphabet[j]\n",
    "        alphabet_pairs.append(pair)\n",
    "\n",
    "len(alphabet_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now test your code with the following interactive word classification script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Training classifier (Smoothing with Laplace estimate) ...\n",
      "Training error (Smoothing with Laplace estimate): 11.25%\n",
      "Test error (Smoothing with Laplace estimate): 14.75%\n",
      "exit\n",
      "exit, is an english word.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' result of the Naive Bayes classifier using pairs of letters as features '''\n",
    "DIMS = 676\n",
    "print('Loading data ...')\n",
    "Xp,Yp = genFeatures(676, language2features_pairs, \"german_train.txt\", \"english_train.txt\")\n",
    "xTe, yTe = genFeatures(676, language2features_pairs, \"german_test.txt\", \"english_test.txt\")\n",
    "print('Training classifier (Smoothing with Laplace estimate) ...')\n",
    "w,b=naivebayesCL(Xp,Yp,naivebayesPXY_smoothing)\n",
    "train_error = np.mean(classifyLinear(Xp,w,b) != Yp)\n",
    "print('Training error (Smoothing with Laplace estimate): %.2f%%' % (100 * train_error))\n",
    "test_error = np.mean(classifyLinear(xTe,w,b) != yTe)\n",
    "print('Test error (Smoothing with Laplace estimate): %.2f%%' % (100 * test_error))\n",
    "\n",
    "yourword = \"\"\n",
    "while yourword!=\"exit\":\n",
    "    yourword = input()\n",
    "    if len(yourword) < 1:\n",
    "        break\n",
    "    xtest = language2features(yourword,B=DIMS,LoadFile=False)\n",
    "    pred = classifyLinear(xtest,w,b)[0]\n",
    "    if pred > 0:\n",
    "        print(\"%s, is an english word.\\n\" % yourword)\n",
    "    else:\n",
    "        print(\"%s, is a german word.\\n\" % yourword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think we are not running the naÃ¯ve Bayes classifier with the __maximum likelihood estimates__ in the previous cell?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": [],
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 [3.7]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
